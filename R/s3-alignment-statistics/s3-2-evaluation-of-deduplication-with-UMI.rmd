---
title: "s3-2 Evaluation of UMI based deduplication"
author: "Yoichiro Sugimoto"
date: "`r format(Sys.time(), '%d %B, %Y')`"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
output:
   html_document:
     highlight: haddock
     toc: yes
     toc_depth: 2
     keep_md: yes
     fig_width: 5
     fig_height: 5
---


# Overview

Deduplication by UMI was not working as expected for the 5'end-Seq data presumably because the sequence depth exceeds the diversity of UMI per transcription start sites.
To evaluate this, the proportion of reads identified as "duplicate" will be examined in the following settings:

1. Actual library: total_RCC4_noVHL_HIF1B_N_1
2. Simulated library with a reduced read duplication: A simulated library has the same read number as total_RCC4_noVHL_HIF1B_N_1_1 but the reads were randomly sampled from the following libraries:
- total_RCC4_noVHL_HIF1B_N_1
- total_RCC4_noVHL_HIF1B_N_3
- total_RCC4_noVHL_HIF1B_N_4
- total_RCC4_noVHL_HIF1B_H_1
- total_RCC4_noVHL_HIF1B_H_3
- total_RCC4_noVHL_HIF1B_H_4

If UMI based deduplication worked as expected, the proportion of duplicated reads in the library 1 should be larger than that of 2.
However, if the sequence depth exceeds UMI diversity, the proportion should be similar.


# Set-up

```{r load libraries, message = FALSE, warning = FALSE}

library("GenomicAlignments")
library("rtracklayer")

## Specify the number of CPUs to be used
processors <- 6

temp <- sapply(list.files("../functions", full.names = TRUE), source)

```


```{r define directory}

sample.file <- file.path("../../data/sample_data/processed_sample_file.csv")

## Input annotation
annot.dir <- file.path("../../annotation/")
annot.ps.dir <- file.path(annot.dir, "hg38_annotation/processed_data/")
annot.R.file <- list.files(
    annot.ps.dir,
    pattern = glob2rx("*primary_transcript_annotation*.rdata"),
    full.names = TRUE
)
load(annot.R.file)

## Input files
results.dir <- file.path("../../results")

s2.alignment.dir <- file.path(results.dir, "s2-read-alignment")
s2.2.processed.bam.dir <-  file.path(s2.alignment.dir, "s2-2-processed-data")
s2.2.1.tss.bam.dir <- file.path(s2.2.processed.bam.dir, "s2-2-1-tss-bam")
s2.2.2.dedup.tss.bam.dir <- file.path(s2.2.processed.bam.dir, "s2-2-2-dedup-tss-bam")

s3.dir <- file.path(results.dir, "s3-alignment-statistics")
s3.2.dir <- file.path(s3.dir, "s3-2-dedup-evaluation")
s3.2.1.mixed.sample.bam.dir <- file.path(s3.2.dir, "s3-2-1-mixed-sample-bam")
s3.2.2.mixed.sample.dedup.dir <- file.path(s3.2.dir, "s3-2-2-mixed-sample-dedup-bam")

create.dirs(c(
    s3.dir,
    s3.2.dir,
    s3.2.1.mixed.sample.bam.dir,
    s3.2.2.mixed.sample.dedup.dir
))

```


```{r read sample data}

sample.dt <- fread(sample.file)
sample.names <- sample.dt[, sample_name]

```

# Generate simulated libraries with reduced level of read duplications



```{r generate mixed sample bam}

r4.sample.names <- sample.names[grepl("total_RCC4_noVHL_HIF1B_(N|H)", sample.names)]

ref.r4.sample.name <- "total_RCC4_noVHL_HIF1B_N_1"

countReadsFromTssBam <- function(sample.name, bam.dir){

    input.bam <- list.files(
        bam.dir,
        pattern = glob2rx(paste0(sample.name, "*.bam$")),
        full.names = TRUE
    )

    rnum <- countBam(input.bam)$records

    return(rnum)
}


input.rnum <- countReadsFromTssBam(ref.r4.sample.name, s2.2.1.tss.bam.dir)

set.seed(1)

generateMixedSampleBam <- function(read.num, input.sample.names, input.bam.dir, s3.2.1.mixed.sample.bam.dir, mixed.sample.num = 6){

    ## mixed.sample.num: the number of randomly sampled bam files to be tested.
    
    input.bam.files <- list.files(
        input.bam.dir,
        pattern = paste0(
            "(", paste(input.sample.names, collapse = "|"), ").*\\.bam$"),
        full.names = TRUE
    )

    input.bams <- mclapply(
        input.bam.files,
        readGAlignments,
        use.names = TRUE,
        mc.cores = processors
    )

    all.bams <- Reduce(append, input.bams)

    all.bams <- sort(all.bams)
    
    for(i in 1:mixed.sample.num){
        sub.bam <- sample(all.bams, size = read.num, replace = FALSE)

        out.sub.bam.file <- file.path(
            s3.2.1.mixed.sample.bam.dir,
            paste0("mixed.sample_bam_", i, ".bam")
        )

        rtracklayer::export(sub.bam, con = out.sub.bam.file, format = "bam")
    }
    
    return()
}

temp <- generateMixedSampleBam(
    read.num = input.rnum,
    input.sample.names = r4.sample.names,
    input.bam.dir = s2.2.1.tss.bam.dir,
    s3.2.1.mixed.sample.bam.dir = s3.2.1.mixed.sample.bam.dir,
    mixed.sample.num = 10
)

```


# Deduplicate mixed sample bam files

Deduplication of mixed sample bam files will be performed here.
The PCR duplication rate should be lower than the bam files from a single sample.

```{r dedup mixed sample bam}

input.bams <- list.files(s3.2.1.mixed.sample.bam.dir, pattern = "bam$", full.names = TRUE)

dedupMixedSampleBam <- function(input.bam, s3.2.2.mixed.sample.dedup.dir){

    output.bam <- file.path(
        s3.2.2.mixed.sample.dedup.dir,
        gsub(".bam", ".dedup.tss.bam", basename(input.bam))
    )

    umi.dedup.cmd <- paste(
        "umi_tools",
        "dedup",
        "-I", input.bam,
        ## paste0("--output-stats=", dedup.stats.out),
        "-S", output.bam
    )

    dedup.out <- system.cat(umi.dedup.cmd)

    return(dedup.out)
}

dedup.outs <- mclapply(
    input.bams,
    dedupMixedSampleBam,
    s3.2.2.mixed.sample.dedup.dir = s3.2.2.mixed.sample.dedup.dir,
    mc.cores = processors
)


```


# Comparison of the proportion of unique reads in the actual data and the simulated high complexity data


```{r comparison of actual and simulated data}

## Reference data
input.dedup.rnum <- countReadsFromTssBam(ref.r4.sample.name, s2.2.2.dedup.tss.bam.dir)

sim.bams <- gsub(
    "\\.bam$",
    "",
    list.files(s3.2.2.mixed.sample.dedup.dir, pattern = "bam$", full.names = FALSE)
)

simulated.dedup.rnums <- mclapply(
    sim.bams,
    countReadsFromTssBam,
    bam.dir = s3.2.2.mixed.sample.dedup.dir,
    mc.cores = processors
)


input.unique.ratio <- input.dedup.rnum / input.rnum 

sim.unique.ratios <- sapply(
    simulated.dedup.rnums, "/",
    input.rnum
)

ratio.dt <- data.table(
    unique_ratio = c(input.unique.ratio, sim.unique.ratios),
    lib = c(ref.r4.sample.name, rep("simulated_complex", times = length(sim.unique.ratios)))
)

ratio.summary.dt <- data.table(
    lib = factor(c(ref.r4.sample.name, "simulated_complex"), levels = c(ref.r4.sample.name, "simulated_complex")),
    unique_ratio = c(input.unique.ratio, mean(sim.unique.ratios)),
    stdev = c(NA, sd(sim.unique.ratios))
)

ggplot(ratio.summary.dt, aes(x = lib, y = unique_ratio)) +
    geom_bar(
        width = 0.5,
        stat = "identity"
    ) +
    geom_errorbar(aes(ymin = unique_ratio - stdev, ymax = unique_ratio + stdev),
                  width = 0.3) +
    ggtitle("Proportion of unique reads") +
    expand_limits(y = 0) +
    theme_classic(16)

```



# Session information


```{r sessionInfo}

sessionInfo()

```



